---
title: "Assignment1"
author: "Luca Cavellini, Paul Hosek, Robert Satzger"
date: "2/12/2023"
output: html_document
---

```{r, include=FALSE}
source("../utilities.R")
```

# Exercise 1. Birthweights
**a)**

To test for a normal distribution of the sample, the output below show a histogram, a Q-Q plot, and a Shapiro-Wilk test.
```{r}
library(ggplot2)
birthweight <- read.table('data/birthweight.txt', header = TRUE)
shapiro <- check_normality("birthweight", birthweight)
```

The histogram appears to have a bell-shaped curve with no dominant skew to either side, suggesting a normal distribution of the sample data. This is also supported by the roughly linear shape displayed in the Q-Q plot. Moreover, the Shapiro-Wilk test for normality is not significant with $p = `r sprintf('%.2f',shapiro[[1]]$p.value)`$. Thus, we cannot reject the null hypothesis stating that the sample differs from a normal distribution. Based on these three indications, we therefore assume that the birthweights follow a normal distribution.

```{r}
birthweight <- birthweight$birthweight
alpha = .04
t = qt(1-alpha/2, df = length(birthweight)-1) # t of alpha/2

margin = t * sd(birthweight) / sqrt(length(birthweight)) 
lower_bound = mean(birthweight) - margin
upper_bound = mean(birthweight) + margin
```
Based on our sample, the 96\% confidence interval for $\mu$ is $CI = [`r sprintf('%.2f',lower_bound)`,`r sprintf('%.2f',upper_bound)`]$.

To obtain the minimal sample size for a confidence interval of length 100, we use the z-distribution as an approximation for the t-distribution as the t-distribution would rely on the sample size.
```{r}
alpha = .04
ci_length = 100
margin = ci_length / 2 
q = qnorm(1-alpha/2) # approximating with z-scores instead of t-values because t-distribution depends on sample size itself
n = q^2 * sd(birthweight)^2 / margin^2 
ceiling(n)
```
Ensuring a confidence interval with a maximum length of 100 requires a sample size $n \geq `r ceiling(n)`$.

```{r}
alpha = .04
B = 1000
Tstar = numeric(B)
for (i in 1:B){
  Tstar[i] = mean(sample(birthweight, replace = TRUE))
}
TstarLower = quantile(Tstar, alpha/2)
TstarUpper = quantile(Tstar, 1-alpha/2)
upper_bound = 2*mean(birthweight)-TstarUpper
lower_bound = 2*mean(birthweight)-TstarLower
```

The 96% confidence interval resulting from bootstrapping is $CI = [`r sprintf('%.2f',upper_bound)`,`r sprintf('%.2f',lower_bound)`]$. As expected, the boundaries of the bootstrapped confidence interval are very similar to the calculated confidence interval reported prior because the sample follows an approximately normal distribution.

**b)**

Based on the assumption of normality established in part **a**, we perform a right-sided one-sample t-test.
```{r}
mu = 2800
ttest <- t.test(birthweight, mu = mu, alternative = 'g')
print(ttest)
```
The t-test returns $p = `r sprintf('%.3f',ttest$p.value)`$. This means we reject the null hypothesis $H0: \mu <= 2800$. The expert's claim is therefore justified. Moreover, the given confidence interval suggests that the population mean lies anywhere higher than `r sprintf('%.2f',ttest$conf.int[[1]])`. In 95% of replications, it will contain the population mean. The undefined upper boundary (displayed as "Inf") is a consequence of the right-sided test - we are only interested in the lower boundary of the CI.

Alternatively, the claim can be tested with a sign test by comparing the proportion of birth weights above 2800 to $H0: p \leq 0.5$.

```{r}
binomtest <- binom.test(sum(birthweight>mu),length(birthweight),p=.5,alt='g')
print(binomtest)
```

Based on the p-value of $p = `r sprintf('%.3f',binomtest$p.value)`$, we can reject the null hypothesis. In accordance with the t-test, the binomial test supports the expert's claim.

**c)** 

The power of the two tests can be compared by simulation.
To do so, one needs to repeatedly sample a normal distribution lying within the alternative hypothesis (with $\mu > 2800$). Each of the samples $X^\ast_b$ then produce a sample mean $\overline{X}^\ast_b$ which is tested against the true mean with both the t-test and the sign test.
Finally, the power of each test is found by calculating the proportions of times each test correctly rejected the null hypothesis.

It can be expected that the t-test will have a higher power than the sign test because it correctly assumes a normal distribution of the sample. In contrast, the sign test discards a lot of information. This makes it more robust than the t-test but reduces power in cases like these where assumptions aren't violated.

**d)**

Deriving the point estimate $\hat{p}$ from our sample, we can use the margin of error to compute the right side of the confidence interval. 
```{r, collapse=TRUE}
p_hat = sum(birthweight<2600)/length(birthweight)
p_left = .25
me = p_hat - p_left
p_right = p_hat + me
z = (p_left - p_hat) / sqrt(p_hat*(1-p_hat)/length(birthweight))
conf_level = 1-pnorm(z)*2
```

This results in the confidence interval $CI = [`r sprintf('%.2f',p_left)`,`r sprintf('%.2f',p_right)`]$. The interval has a confidence level of $`r sprintf('%.0f',conf_level*100)`\%$.

**e)**

Since we are not given the distribution of birth weights by gender, we cannot perform a test for difference in means. Thus, we perform a test for difference in proportions. 
```{r}
k_male = 34
k_female = 28
n_male = k_male + 61
n_female = k_female + 65
p_female = k_female/n_female

proptest = prop.test(c(k_male,k_female), c(n_male, n_female))
print(proptest)
```

The proportion test returns $p = `r sprintf('%.2f',proptest$p.value)`$. Since it is larger than the standard significance level, we reject the null hypothesis stating that the two proportions are significantly different. Thus, the expert's claim is not supported by the data.
